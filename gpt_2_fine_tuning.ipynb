{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+5MYWYw2mY3icJy6BuaHK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arishp/veltech_genai/blob/main/gpt_2_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets accelerate torchvision"
      ],
      "metadata": {
        "id": "2g68oGsMXCBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "cKZFpnhwWPss"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://huggingface.co/datasets/hakurei/open-instruct-v1\n",
        "dataset = load_dataset(\"hakurei/open-instruct-v1\", split='train')\n",
        "print(dataset.to_pandas().sample(5))"
      ],
      "metadata": {
        "id": "eNGMOF_TXWqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(example):\n",
        "    example['prompt'] = f\"{example['instruction']} {example['input']} {example['output']}\"\n",
        "    return example"
      ],
      "metadata": {
        "id": "n9_ufrNPXL6f"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Before preprocessing: {dataset}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHPf5U3DYV3c",
        "outputId": "ee1d44ca-2b7a-4999-c40d-026e9e48603c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before preprocessing: Dataset({\n",
            "    features: ['output', 'instruction', 'input'],\n",
            "    num_rows: 498813\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(preprocess, remove_columns=['instruction', 'input', 'output'])\n",
        "print(f\"After preprocessing: {dataset}\")"
      ],
      "metadata": {
        "id": "i_TlJlUGYbtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.shuffle(seed=42).select(range(100000)).train_test_split(test_size=0.1)\n",
        "print(f\"After train test split: {dataset}\")"
      ],
      "metadata": {
        "id": "OiqQY7eeYmZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = dataset['train']\n",
        "test_dataset = dataset['test']"
      ],
      "metadata": {
        "id": "GzZTW-QMYtlY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://huggingface.co/microsoft/DialoGPT-medium\n",
        "MODEL_NAME = 'microsoft/DialoGPT-medium'\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "vMD8_d1CYx4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"First row Before tokenizing: {train_dataset['prompt'][0]}\")"
      ],
      "metadata": {
        "id": "ARj0vAXlY6Ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_dataset(dataset):\n",
        "    tokenized_dataset = dataset.map(lambda example:tokenizer(example['prompt'], truncation=True, max_length=128), batched=True, remove_columns=['prompt'])\n",
        "    return tokenized_dataset"
      ],
      "metadata": {
        "id": "H0zWApKdXR00"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tokenize_dataset(train_dataset)\n",
        "test_dataset = tokenize_dataset(test_dataset)\n",
        "print(f\"tokenized train dataset: {train_dataset}\")\n",
        "print(f\"First row After tokenizing: {train_dataset['input_ids'][0], train_dataset['attention_mask'][0]}\")"
      ],
      "metadata": {
        "id": "AYZX6s3YYVLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./dialogpt2-instruct',\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=4,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    data_collator=data_collator,\n",
        ")"
      ],
      "metadata": {
        "id": "JQYTkhpvZFK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"training started...\")\n",
        "trainer.train()\n",
        "print(\"training completed...\")\n",
        "trainer.save_model()\n",
        "print(\"saved model...\")"
      ],
      "metadata": {
        "id": "V74RpJQmcVI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fine_tuned_model = AutoModelForCausalLM.from_pretrained('./dialogpt2-instruct').to('cuda')\n",
        "fine_tuned_model = AutoModelForCausalLM.from_pretrained('TheFuzzyScientist/diabloGPT_open-instruct') #.to('cuda')"
      ],
      "metadata": {
        "id": "DTdsh-L6ce6I"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(prompt, model_selected):\n",
        "    inputs = tokenizer.encode(prompt, return_tensors='pt') #.to('cuda')\n",
        "    outputs = model_selected.generate(inputs, max_length=64, pad_token_id=tokenizer.eos_token_id)\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_token=True, use_mps_device=True)\n",
        "    return generated_text[: generated_text.rfind('.')+1]"
      ],
      "metadata": {
        "id": "DNopNTH_dXy8"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generating text from base model... \")\n",
        "print(generate_text(\"I like to drink...\", model))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sJLVHHtd0J-",
        "outputId": "9f022b8e-bfe0-4957-d0d7-0ca2a6c4cf93"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating text from base model... \n",
            "I like to drink...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generating text from fine tuned model... \")\n",
        "print(generate_text(\"I like to drink...\", fine_tuned_model))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9sLgQjsd3br",
        "outputId": "0ac2c26f-c784-46ba-8e88-79ebfa5f90ee"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating text from fine tuned model... \n",
            "I like to drink...  I like to drink coffee.  I like to drink tea.  I like to drink coffee with milk.  I like to drink tea with milk.  I like to drink coffee with milk.  I like to drink tea with milk.  I like to drink coffee with milk.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hw_v0Q_jeDLz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}