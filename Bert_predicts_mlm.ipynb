{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTfbuJLJqxy0vAGCJG9iS4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arishp/veltech_genai/blob/main/Bert_predicts_mlm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wb-EDf8nRND"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "from scipy.special import softmax\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the pre-trained model to use: BERT-base-cased\n",
        "model_name = \"bert-base-cased\"\n",
        "\n",
        "# Instantiate the tokenizer and model for the specified pre-trained model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdvG33dOngSJ",
        "outputId": "11ce965b-6b1c-4c2d-9ab0-b2a8f5152cca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the mask token from the tokenizer\n",
        "mask = tokenizer.mask_token\n",
        "print(mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUMmmFYonoBf",
        "outputId": "0841fa37-f73e-4c7b-a1bd-c89b79ff051a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MASK]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sentence with a mask token to be filled in by the model\n",
        "sentence = f\"I want to {mask} pizza for tonight.\"\n",
        "# Tokenize the sentence\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKKocs2bnySY",
        "outputId": "2a0a201e-afbf-47b3-a1d4-2797120bd851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'want', 'to', '[MASK]', 'pizza', 'for', 'tonight', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the sentence using the tokenizer and return the input tensors\n",
        "encoded_inputs = tokenizer(sentence, return_tensors='pt')\n",
        "print(encoded_inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4w9HfcQn97l",
        "outputId": "cae8f8fc-8fdb-4688-a60e-ba45ba4f878f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101,   146,  1328,  1106,   103, 13473,  1111,  3568,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the model's output for the input tensors\n",
        "outputs = model(**encoded_inputs)\n",
        "print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTmVSgwgoJsK",
        "outputId": "45ac4061-1814-47cb-f01e-5322672d65f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MaskedLMOutput(loss=None, logits=tensor([[[ -7.3723,  -7.2489,  -7.4421,  ...,  -6.3119,  -5.9369,  -6.4257],\n",
            "         [ -7.9311,  -8.2282,  -8.0326,  ...,  -6.7387,  -6.4877,  -6.9525],\n",
            "         [-12.0500, -11.7972, -12.5776,  ...,  -8.4518,  -6.7310,  -8.2586],\n",
            "         ...,\n",
            "         [-10.2204, -10.4315,  -9.9993,  ...,  -7.9570,  -6.7194,  -9.3618],\n",
            "         [-12.4471, -12.5367, -12.5614,  ...,  -9.9086,  -9.4219, -11.1770],\n",
            "         [-14.3657, -14.5227, -15.0017,  ..., -11.9715, -11.6569, -13.4498]]],\n",
            "       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Detach the logits from the model's output and convert them to numpy arrays\n",
        "logits = outputs.logits.detach().numpy()[0]\n",
        "logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBQdEpecoQn9",
        "outputId": "677b1287-0b79-4aa0-c381-463591fa6aa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 28996)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UF2QcCao0T2",
        "outputId": "a73bbc03-48de-4727-9969-4011a30f68e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the logits for the mask token\n",
        "mask_logits = logits[tokens.index(mask) + 1]\n",
        "print(mask_logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeVCIFmPonho",
        "outputId": "d0e06243-3e2c-4984-8004-672cd3183802"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-6.7146263 -6.3791075 -6.1184874 ... -5.651307  -3.657276  -4.994728 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the confidence scores for each possible token using softmax\n",
        "confidence_scores = softmax(mask_logits)\n",
        "print(confidence_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0M75hHJyo_SZ",
        "outputId": "8fcfab95-5d11-4200-f4a7-67b5b6301e69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2.9159986e-10 4.0785036e-10 5.2928162e-10 ... 8.4446450e-10 6.2026548e-09\n",
            " 1.6282821e-09]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "confidence_scores.sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRBz8aehpJop",
        "outputId": "d3435025-38c4-4687-cd12-391828627db4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0000001"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the top 5 predicted tokens and their confidence scores\n",
        "for i in np.argsort(confidence_scores)[::-1][:5]:\n",
        "    pred_token = tokenizer.decode(i)\n",
        "    score = confidence_scores[i]\n",
        "\n",
        "    # Print the predicted sentence with the mask token replaced by the predicted token, and the confidence score\n",
        "    print(sentence.replace(mask, pred_token), score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Da6XIM3mpFyk",
        "outputId": "ea021447-5be9-4ecd-c224-10e747c3a6f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I want to have pizza for tonight. 0.25729004\n",
            "I want to get pizza for tonight. 0.17849584\n",
            "I want to eat pizza for tonight. 0.1555555\n",
            "I want to make pizza for tonight. 0.11422437\n",
            "I want to order pizza for tonight. 0.09823046\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "azb6n0kbpSVZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}